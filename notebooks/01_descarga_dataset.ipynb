{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce2765a",
   "metadata": {},
   "source": [
    "# üìò Descarga de Dataset desde NASA\n",
    "Este notebook descarga archivos `.zip` del dataset de turbinas desde el sitio web de la NASA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL del sitio\n",
    "url = 'https://www.nasa.gov/intelligent-systems-division/discovery-and-systems-health/pcoe/pcoe-data-set-repository'\n",
    "response = requests.get(url)\n",
    "print('üì° Estado de la respuesta:', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45023ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c181cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios\n",
    "page_path = 'data/raw/page.html'\n",
    "csv_path = 'data/raw/links.csv'\n",
    "output_dir = 'data/raw'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar p√°gina HTML\n",
    "with open(page_path, 'w', encoding='utf-8') as page:\n",
    "    page.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar enlaces .zip\n",
    "tags = soup.find_all('a', href=True)\n",
    "enlaces = []\n",
    "for tag in tags:\n",
    "    text = tag.get_text(strip=True)\n",
    "    href = tag['href']\n",
    "    if href.endswith('.zip') and \"turbofan\" in href.lower():\n",
    "        enlaces.append((text, href))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49de68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar links en CSV\n",
    "with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Texto', 'Enlace'])\n",
    "    writer.writerows(enlaces)\n",
    "\n",
    "print(f\"üóÑÔ∏è Se guardaron {len(enlaces)} enlaces en links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar archivos\n",
    "with open(csv_path, encoding='utf-8') as fr:\n",
    "    reader = csv.reader(fr)\n",
    "    next(reader)\n",
    "    for text, url in reader:\n",
    "        filename = unquote(url.split('/')[-1]).strip()\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        print(f\"‚¨áÔ∏è Descargando: {filename}\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(file_path, 'wb') as f_out:\n",
    "                for chunk in r.iter_content(8192):\n",
    "                    f_out.write(chunk)\n",
    "            print(f\"‚úÖ Guardado en: {file_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úñÔ∏è Error al descargar {url}: {e}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
